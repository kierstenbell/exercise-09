---
title: "Exercise 09"
output: html_document
date: "2024-03-18"
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Step 1 
- Using the {tidyverse} read_csv() function, load the “Street_et_al_2017.csv” dataset from this URL as a “tibble” named d.
- Do a quick exploratory data analysis where you generate the five-number summary (median, minimum and maximum and 1st and 3rd quartile values), plus mean and standard deviation, for each quantitative variable.

```{r echo=TRUE, message=FALSE}
# Set up libraries
library(tidyverse)
library(skimr)
library(tidyr)
library(broom)
library(infer)
```

```{r, 1}
f <- "https://raw.githubusercontent.com/difiore/ada-2024-datasets/main/Street_et_al_2017.csv"
d <- read_csv(f, col_names = TRUE)

# Quick glance
head(d)

# From {skimr}; provides n_missing, mean, sd, and five number summary (min, max, Q1, median, Q3)
skim(d)

# Make it a tibble 
# (tibble_d <- skim(d) 
#  tibble::as_tibble())

# You can also easily use group_by function to ...group...by... variables
# d %>%
#  dplyr::group_by(Species) %>%
#  skim()

```

### Step 2 
- From this dataset, plot brain size (ECV) as a function of social group size (Group_size), longevity (Longevity), juvenile period length (Weaning), and reproductive lifespan (Repro_lifespan).

```{r, 2}

par(mfrow = c(2, 2))
plot(data = d, ECV ~ Group_size, main = "Social Group Size", ylab = "Brain Size (ECV)")
plot(data = d, ECV ~ Longevity, main = "Longevity", ylab = "Brain Size (ECV)")
plot(data = d, ECV ~ Weaning, main = "Juvenile Period Length", ylab = "Brain Size (ECV)")
plot(data = d, ECV ~ Repro_lifespan, main = "Reproductive Lifespan", ylab = "Brain Size (ECV)")
```

### Step 3
- Derive by hand the ordinary least squares regression coefficients $\beta1$ and $\beta0$ for ECV as a function of social group size

```{r, 3}
# I do not understand why this yields diff results; B1 = 2.718; B0 = -172.906
# Solve for Beta1, slope, by hand
# (B1 <- cor(d$ECV, d$Group_size, use = "complete.obs") * (sd(d$ECV, na.rm = TRUE)/sd(d$Group_size, na.rm = TRUE)))

# Solve for Beta0, intercept, by hand 
# (B0 <- mean(d$ECV, na.rm = TRUE) - B1*mean(d$Group_size, na.rm = TRUE)) 

# Removing rows in ECV and Group_size that are not complete
ECV_GS_noNA <- d[complete.cases(d[ , c('ECV', 'Group_size')]), ]

# Rename the variables 
ECV <- ECV_GS_noNA$ECV
Group_size <- ECV_GS_noNA$Group_size

# Solve for Beta1 (slope) by hand
(beta1 <- cor(ECV, Group_size) * (sd(ECV)/sd(Group_size)))

# Solve for Beta0 (intercept) by hand
(beta0 <- mean(ECV) - beta1 * mean(Group_size))

```

### Step 4
- Confirm that you get the same results using the lm() function 

```{r, 4}
(ECV_GS_lm <- lm(ECV ~ Group_size, data = d))
```


### Step 5
- Repeat the analysis above for three different major radiations of primates - “catarrhines”, “platyrrhines”, and “strepsirhines”) separately. These are stored in the variable Taxonomic_group. Do your regression coefficients differ among groups? How might you determine this?
- The intercepts (B0) vary between the groups, but the slopes (B1) are more similar. 


```{r, 5}
cat <- filter(ECV_GS_noNA, Taxonomic_group == "Catarrhini")
cat_lm <- lm(ECV ~ Group_size, data = cat)
plat <- filter(ECV_GS_noNA, Taxonomic_group == "Platyrrhini")
plat_lm <- lm(ECV ~ Group_size, data = plat)
strep <- filter(ECV_GS_noNA, Taxonomic_group == "Strepsirhini")
strep_lm <- lm(ECV ~ Group_size, data = strep)

# Combine results into a single data frame
# from {tidyr} and {broom}
cat_lm <- tidy(cat_lm) %>%
  mutate(Primate = "Catarrhini")
plat_lm <- tidy(plat_lm) %>%
  mutate(Primate = "Platyrrhini")
strep_lm <- tidy (strep_lm) %>%
  mutate(Primate = "Strepsirhini")
(primate_lm <- bind_rows(cat_lm, plat_lm, strep_lm, .id = "Model"))

```

### Step 6
- For your first regression of ECV on social group size, calculate the standard error for the slope coefficient, the 95% CI, and the p value associated with this coefficient by hand. Also extract this same information from the results of running the lm() function.

```{r, 6}
# Summary 
summary <- tidy(ECV_GS_lm)

# standard error 
summary$calc.statistic <- (summary$estimate - 0)/summary$std.error

# 95% confidence interval 
alpha <- 0.05
lower <- summary$estimate - qt(1 - alpha/2, df = nrow(ECV_GS_noNA) - 2) *summary$std.error
upper <- summary$estimate + qt(1 - alpha/2, df = nrow(ECV_GS_noNA) - 2) *summary$std.error
CI <- cbind(lower, upper)
rownames(CI) <- c("(Intercept)", "Group size")
colnames(CI) <- c(paste0(as.character(alpha/2 * 100), " %"), paste0(as.character((1 -
    alpha/2) * 100), " %"))
CI

# p value
summary$calc.p.value <- 2 * pt(summary$calc.statistic, df = nrow(ECV_GS_noNA) - 2, lower.tail = FALSE)

# print summary
summary

# Using lm() function
# Completed in step 4
summary(ECV_GS_lm)
glance(ECV_GS_lm)
```

### Step 7
- Use a permutation approach with 1000 permutations to generate a null sampling distribution for the slope coefficient. What is it that you need to permute? What is the p value associated with your original slope coefficient? You can use either the percentile method (i.e., using quantiles from the actual permutation-based null sampling distribution) or a theory-based method (i.e., using the standard deviation of the permutation-based null sampling distribution as the estimate of the standard error, along with a normal or t distribution), or both, to calculate this p value.

```{r, 7}
# define variables
alpha <- 0.05
confidence_level <- 1 - alpha
p_lower <- alpha/2
p_upper <- 1 - (alpha/2)
degrees_of_freedom <- nrow(ECV_GS_noNA) - 2
critical_value <- qt(p_upper, df = degrees_of_freedom) # qt() is the student t distribution

# original slope
original.slope <- lm(data = ECV_GS_noNA, ECV ~ Group_size) %>%
  # tidy the model and add the CI based on the t distribution 
  tidy(conf.int = TRUE, conf.level = confidence_level) %>%
  #alternative: mutate(lower = estimate - std.error * critical_value, upper = estimate + std.error * critical_value)
  filter(term == "Group_size")
original.slope

# null distribution 
permuted.slope <- ECV_GS_noNA %>%
  # specify model 
  specify(ECV ~ Group_size) %>%
  # use a null hypothesis of independence
  hypothesize(null = "independence") %>%
  # generate permutation replicates
  generate(reps = 1000, type = "permute") %>%
  # calculate the slope statistic
  calculate(stat = "slope") # from {infer}

# create confidence intervals
permuted.slope.summary <- permuted.slope %>%
  # summarize the mean, t distribution based CI, and quantile-based CI
  summarize(
    # mean of stat
    estimate = mean(stat), # should be very close to ZERO
    # std error of stat
    std.error = sd(stat),
    # calculate the CI based on the SE and t distribution
    # could use 0 in place of estimate here...
    lower = estimate - std.error * critical_value,
    upper = estimate + std.error * critical_value,
    # calculate the CI based on the quantile (percentile) method
    perm.lower = quantile(stat, p_lower),
    perm.upper = quantile(stat, p_upper)
  )

# show summary of permuted sampling distribution
permuted.slope.summary
```

### Step 8 
- Use bootstrapping to generate a 95% CI for your estimate of the slope coefficient using both the percentile method and the theory-based method (i.e., using on the standard deviation of the bootstrapped sampling distribution as an estimate of the standard error). Do these CIs suggest that your slope coefficient is different from zero?

```{r, 8}


```
